# COMPAS Recidivism Algorithm Bias Audit Report

**Dataset**: 7,214 criminal cases from ProPublica's COMPAS analysis  
**Analysis Date**: November 26, 2025  
**Analyst**: AI Ethics Audit Framework

## Executive Summary

This audit examines racial bias in the COMPAS recidivism risk assessment algorithm. The analysis reveals **severe disparities** in algorithmic outcomes across racial groups, with African American defendants experiencing systematic disadvantage in risk assessments.

## Key Findings

### 1. Demographic Parity Violation
African American defendants are classified as high-risk (score â‰¥5) at a rate of **58.8%**, compared to **32.5%** for other racial groups. This represents a disparate impact ratio of **1.81**, far exceeding the legally accepted 80%-125% threshold, indicating substantial bias against African Americans.

### 2. Equal Opportunity Violation  
False positive rates show disturbing disparity: **44.8%** for African Americans versus **22.0%** for others. This **22.8 percentage point difference** means African Americans who won't reoffend are twice as likely to receive high-risk classifications, increasing their chances of harsher sentencing or detention.

### 3. Predictive Performance Gaps
The algorithm demonstrates unequal accuracy across racial groups, with true positive rates of **72.0%** for African American defendants versus **49.3%** for others. This inconsistency suggests the algorithm's predictive power varies significantly by race.

### 4. Systematic Bias Indicators
- African Americans receive **81% higher** high-risk classifications
- **51.4%** actual recidivism rate for African Americans vs **39.4%** for Caucasians
- Average COMPAS scores: African Americans (5.37) vs Others (3.54)

## Remediation Recommendations

**Immediate Actions**: Implement continuous bias monitoring with automated alerts when disparate impact exceeds thresholds. Establish diverse oversight committee including affected community representatives.

**Algorithmic Improvements**: Retrain models with fairness constraints (equalized odds, demographic parity) and implement adversarial debiasing to remove racial correlations from risk assessments.

**Data-Level Interventions**: Audit historical training data for bias patterns and implement balanced sampling across demographic groups.

**Process Safeguards**: Require human review for high-risk classifications and provide transparent explanations for risk scores.

## Conclusion

The COMPAS algorithm perpetuates significant racial bias in criminal justice decisions. With African Americans experiencing disproportionately higher false positive rates and being 81% more likely to receive high-risk classifications, immediate intervention is required to ensure equal protection under the law and prevent discriminatory outcomes in sentencing and detention decisions.

This analysis provides compelling evidence for regulatory oversight and algorithmic reform to achieve fairness in criminal justice risk assessment.